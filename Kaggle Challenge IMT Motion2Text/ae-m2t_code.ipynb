{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccc7e7cb-3f6e-4da2-bfb2-076328f3e906",
    "tags": []
   },
   "source": [
    "---\n",
    "## **<p style=\"text-align: center; text-decoration: underline;\">DATA CHALLENGE</p>**\n",
    "# **<p style=\"text-align: center;\">HUMAN MOTION DESCRIPTION (HMD): Motion-To-Text</p>**\n",
    "---\n",
    "\n",
    "> IMT Nord Europe *2025*.\n",
    "\n",
    "---\n",
    "\n",
    "![examples](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fimg.clipart-library.com%2F2%2Fclip-motions%2Fclip-motions-6.png&f=1&nofb=1&ipt=0747ffa645bb5f7798e8a2d44499b28f1156ce0e83b1b300fabfed4c6ab1fdf2&ipo=images)\n",
    "\n",
    "### ■ **Overview**\n",
    "In this data challenge, you will explore the intersection of natural language processing (NLP) and human motion synthesis by working on text-to-motion and motion-to-text tasks using the HumanML3D dataset. This dataset contains 3D human motion sequences paired with rich textual descriptions, enabling models to learn bidirectional mappings between language and motion.\n",
    "\n",
    "#### **I. Main Task: Motion-To-Text Generation**\n",
    "- **Motion-to-Text:** Develop a model to describe human motions in natural language given a sequence of 3D poses.\n",
    "\n",
    "#### **II. Dataset Overview:**\n",
    "- HumanML3D includes 14,616 motion samples across diverse actions (walking, dancing, sports) and 44,970 text annotations.\n",
    "- Data includes skeletal joint positions, rotations, and fine-grained textual descriptions.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fproduction-media.paperswithcode.com%2Fdatasets%2F446194c5-ce59-43eb-b4cb-570a7a4d0cd9.png&f=1&nofb=1&ipt=b2edbe3251cab88e26a7f9d4e765c811b2cc890dc2ace7f7456baeca076b115b&ipo=images\" alt=\"description\" style=\"width:800px; height:600px;\" />\n",
    "\n",
    "The provided dataset contains the following components:\n",
    "\n",
    "- 1. `motions` Folder: Contains `.npy` files, each representing a sequence of body poses. Each file has a shape of `(T, N, d)`, where:\n",
    "  - `T`: Number of frames in the sequence (varies across sequences).\n",
    "  - `N`: Number of joints in the body (22 in this case).\n",
    "  - `d`: Dimension of each joint (3D coordinates: `x`, `y`, `z`).\n",
    "\n",
    "- 2. `texts` Folder: Contains `.npy` files, each providing **3 textual descriptions** of the corresponding motion sequence. Each description is accompanied by part-of-speech (POS) tags for every word in the description. Example: \"a person jump hop to the right#a/DET person/NOUN jump/NOUN hop/NOUN to/ADP the/DET right/NOUN#\"\n",
    "\n",
    "- 3. File Lists\n",
    "    - **`all.txt`**: List of all motion files in the dataset.\n",
    "    - **`train.txt`**: List of motion files for training.\n",
    "    - **`val.txt`**: List of motion files for validation.\n",
    "    - **`test.txt`**: List of motion files for testing.\n",
    "\n",
    "\n",
    "#### **III. Evaluation Metrics**\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy): The BLEU score evaluates the quality of generated text against reference texts using n-gram precision.\n",
    "> Note: Higher BLEU scores (closer to 1 or 100\\%) indicate better text-motion alignment. BLEU focuses on lexical overlap, not semantic accuracy. For motion descriptions, it measures how well generated text matches the linguistic patterns of ground-truth annotations.\n",
    "\n",
    "Solutions should be submitted in the following format (in a csv file):\n",
    "\n",
    "For each ID in the motion test set, you must predict the corresponding description. The file should contain a header and have the following format:\n",
    "\n",
    "| id      | text                                                                 |\n",
    "|---------|---------------------------------------------------------------------|\n",
    "| 004822  | A person walks slowly forward, swinging their arms naturally        |\n",
    "| 014457  | Someone performs a golf swing with proper form                      |\n",
    "| 009613  | An individual jogs backwards diagonally across the room             |\n",
    "| 008463  | A man bends down to pick up an object while walking                 |\n",
    "| 012365  | A dancer spins clockwise while raising both arms                    |\n",
    "| 007933  | Two people engage in a slow-motion martial arts demonstration       |\n",
    "| 003430  | A child skips happily across a playground                           |\n",
    "| 014522  | An athlete performs a perfect cartwheel sequence                    |\n",
    "| 005698  | A woman gracefully practices yoga sun salutations                   |\n",
    "| 001664  | A parkour expert vaults over a low wall                             |\n",
    "\n",
    "You can generate your submission files using pandas as follows:\n",
    "\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'id': ['004822', '014457', ...],\n",
    "    ...     'text': [\n",
    "    ...         \"a person walking slowly\",\n",
    "    ...         \"someone swinging a golf club\",\n",
    "    ...         ...\n",
    "    ...     ]\n",
    "    ... })\n",
    "    ... submission.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Auto Encoder Motion2Text by Antoine Mariot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:08:33.291370Z",
     "iopub.status.busy": "2025-01-31T06:08:33.291090Z",
     "iopub.status.idle": "2025-01-31T06:08:33.298292Z",
     "shell.execute_reply": "2025-01-31T06:08:33.297343Z",
     "shell.execute_reply.started": "2025-01-31T06:08:33.291346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:08:33.781145Z",
     "iopub.status.busy": "2025-01-31T06:08:33.780681Z",
     "iopub.status.idle": "2025-01-31T06:08:33.785088Z",
     "shell.execute_reply": "2025-01-31T06:08:33.784078Z",
     "shell.execute_reply.started": "2025-01-31T06:08:33.781102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Importation des données\n",
    "motion_data_dir = './motions'\n",
    "text_data_dir = './texts' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:08:34.075217Z",
     "iopub.status.busy": "2025-01-31T06:08:34.074944Z",
     "iopub.status.idle": "2025-01-31T06:08:34.089358Z",
     "shell.execute_reply": "2025-01-31T06:08:34.088654Z",
     "shell.execute_reply.started": "2025-01-31T06:08:34.075196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Chargement des indices depuis un fichier texte\n",
    "def load_indices(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        indices = [line.strip() for line in f.readlines()]\n",
    "    return indices\n",
    "\n",
    "# Charger les indices pour train, test, val\n",
    "train_indices = load_indices(\"./train.txt\")\n",
    "test_indices = load_indices(\"./test.txt\")\n",
    "val_indices = load_indices(\"./val.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement séquences de mouvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:08:35.348384Z",
     "iopub.status.busy": "2025-01-31T06:08:35.348068Z",
     "iopub.status.idle": "2025-01-31T06:08:35.352732Z",
     "shell.execute_reply": "2025-01-31T06:08:35.351924Z",
     "shell.execute_reply.started": "2025-01-31T06:08:35.348355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_motion_sequences(sequence_dir, indices):\n",
    "    sequences = []\n",
    "    for idx in indices:\n",
    "        file_path = f\"{sequence_dir}/{idx}.npy\"\n",
    "        if os.path.exists(file_path):  # Vérifie que le fichier existe\n",
    "            sequences.append(np.load(file_path))\n",
    "        else:\n",
    "            print(f\"Fichier manquant : {file_path}\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:08:38.326153Z",
     "iopub.status.busy": "2025-01-31T06:08:38.325880Z",
     "iopub.status.idle": "2025-01-31T06:08:38.330765Z",
     "shell.execute_reply": "2025-01-31T06:08:38.329907Z",
     "shell.execute_reply.started": "2025-01-31T06:08:38.326131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_sequences(sequences):\n",
    "    all_data = np.concatenate(sequences, axis=0)  # Combine toutes les séquences\n",
    "    mean = all_data.mean(axis=(0, 1))  # Moyenne globale sur frames et jointures\n",
    "    std = all_data.std(axis=(0, 1))   # Écart-type global\n",
    "\n",
    "    # Appliquer la normalisation\n",
    "    normalized_sequences = [(seq - mean) / std for seq in sequences]\n",
    "    return normalized_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:08:38.710877Z",
     "iopub.status.busy": "2025-01-31T06:08:38.710544Z",
     "iopub.status.idle": "2025-01-31T06:08:38.715830Z",
     "shell.execute_reply": "2025-01-31T06:08:38.714736Z",
     "shell.execute_reply.started": "2025-01-31T06:08:38.710852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_to_tensor(sequences, max_length=150):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_length:  # Troncature si la séquence est trop longue\n",
    "            seq = seq[:max_length]\n",
    "        else:  # Padding si la séquence est trop courte\n",
    "            padding = np.zeros((max_length - len(seq), seq.shape[1], seq.shape[2]))\n",
    "            seq = np.vstack([seq, padding])\n",
    "        padded_sequences.append(seq)\n",
    "    \n",
    "    return torch.tensor(padded_sequences, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:08:41.969246Z",
     "iopub.status.busy": "2025-01-31T06:08:41.968936Z",
     "iopub.status.idle": "2025-01-31T06:11:32.311945Z",
     "shell.execute_reply": "2025-01-31T06:11:32.310985Z",
     "shell.execute_reply.started": "2025-01-31T06:08:41.969216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_sequences = load_motion_sequences(motion_data_dir, train_indices)\n",
    "train_sequences = [seq for seq in train_sequences if seq.ndim == 3 and seq.shape[0] > 1]\n",
    "train_sequences = normalize_sequences(train_sequences)\n",
    "train_tensor = convert_to_tensor(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:13:44.869530Z",
     "iopub.status.busy": "2025-01-31T06:13:44.869210Z",
     "iopub.status.idle": "2025-01-31T06:13:53.303285Z",
     "shell.execute_reply": "2025-01-31T06:13:53.302610Z",
     "shell.execute_reply.started": "2025-01-31T06:13:44.869502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "test_sequences = load_motion_sequences(motion_data_dir, test_indices)\n",
    "test_sequences = [seq for seq in test_sequences if seq.ndim == 3 and seq.shape[0] > 1]\n",
    "test_sequences = normalize_sequences(test_sequences)\n",
    "test_tensor = convert_to_tensor(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:14:19.927513Z",
     "iopub.status.busy": "2025-01-31T06:14:19.927224Z",
     "iopub.status.idle": "2025-01-31T06:14:54.002983Z",
     "shell.execute_reply": "2025-01-31T06:14:54.002275Z",
     "shell.execute_reply.started": "2025-01-31T06:14:19.927491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# val\n",
    "val_sequences = load_motion_sequences(motion_data_dir, val_indices)\n",
    "val_sequences = [seq for seq in val_sequences if seq.ndim == 3 and seq.shape[0] > 1]\n",
    "val_sequences = normalize_sequences(val_sequences)\n",
    "val_tensor = convert_to_tensor(val_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:15:26.766198Z",
     "iopub.status.busy": "2025-01-31T06:15:26.765908Z",
     "iopub.status.idle": "2025-01-31T06:15:26.771564Z",
     "shell.execute_reply": "2025-01-31T06:15:26.770895Z",
     "shell.execute_reply.started": "2025-01-31T06:15:26.766173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#vérification\n",
    "print(\"Val tensor:\", val_tensor.shape)\n",
    "print(\"Train tensor:\", train_tensor.shape)\n",
    "print(\"Test tensor:\", test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:15:28.438727Z",
     "iopub.status.busy": "2025-01-31T06:15:28.438419Z",
     "iopub.status.idle": "2025-01-31T06:15:29.633243Z",
     "shell.execute_reply": "2025-01-31T06:15:29.632473Z",
     "shell.execute_reply.started": "2025-01-31T06:15:28.438702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sauvegarge des tenseurs pour futurs imports\n",
    "# Définir les chemins de sauvegarde\n",
    "save_path = \"\"\n",
    "torch.save(val_tensor, save_path + \"val_tensor.pt\")\n",
    "torch.save(train_tensor, save_path + \"train_tensor.pt\")\n",
    "torch.save(test_tensor, save_path + \"test_tensor.pt\")\n",
    "\n",
    "print(\"Tenseurs sauvegardés avec succès !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des tenseurs déjà sauvegardés si besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T20:09:58.586356Z",
     "iopub.status.busy": "2025-01-30T20:09:58.585945Z",
     "iopub.status.idle": "2025-01-30T20:10:07.806265Z",
     "shell.execute_reply": "2025-01-30T20:10:07.805218Z",
     "shell.execute_reply.started": "2025-01-30T20:09:58.586332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Charger les tenseurs\n",
    "train_tensor = torch.load(f\"./train_tensor.pt\")\n",
    "val_tensor = torch.load(f\"./val_tensor.pt\")\n",
    "test_tensor = torch.load(f\"./test_tensor.pt\")\n",
    "\n",
    "print(\"Tenseurs chargés !\")\n",
    "print(f\"Train Tensor: {train_tensor.shape}\")\n",
    "print(f\"Val Tensor: {val_tensor.shape}\")\n",
    "print(f\"Test Tensor: {test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des descriptions textuelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:15:36.167002Z",
     "iopub.status.busy": "2025-01-31T06:15:36.166680Z",
     "iopub.status.idle": "2025-01-31T06:15:36.171913Z",
     "shell.execute_reply": "2025-01-31T06:15:36.171122Z",
     "shell.execute_reply.started": "2025-01-31T06:15:36.166972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_text_descriptions(text_dir, indices, max_sentences=3):\n",
    "    text_descriptions = {}  # Dictionnaire {id: [description1, description2, description3]}\n",
    "\n",
    "    for idx in indices:\n",
    "        file_path = os.path.join(text_dir, f\"{idx}.txt\")\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                lines = [line.split('#')[0].strip() for line in f.readlines() if line.strip()]\n",
    "                \n",
    "                # Garder jusqu'à `max_sentences` phrases\n",
    "                text_descriptions[idx] = lines[:max_sentences]\n",
    "                \n",
    "        else:\n",
    "            print(f\"Avertissement : {idx}.txt introuvable\")\n",
    "    \n",
    "    return text_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:15:37.784108Z",
     "iopub.status.busy": "2025-01-31T06:15:37.783821Z",
     "iopub.status.idle": "2025-01-31T06:15:37.790604Z",
     "shell.execute_reply": "2025-01-31T06:15:37.789843Z",
     "shell.execute_reply.started": "2025-01-31T06:15:37.784087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MotionTextDataset(Dataset):\n",
    "    def __init__(self, motion_dict, text_descriptions_dict, tokenizer, max_length=50, multiple_sentences=False):\n",
    "        \"\"\"\n",
    "        Dataset associant les séquences de mouvements à plusieurs descriptions textuelles.\n",
    "        \"\"\"\n",
    "        self.motions = motion_dict\n",
    "        self.text_descriptions = text_descriptions_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.indices = list(text_descriptions_dict.keys())  # Liste des indices valides\n",
    "        self.max_length = max_length\n",
    "        self.multiple_sentences = multiple_sentences  # Option pour gérer plusieurs phrases\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        \n",
    "        # Récupérer la séquence de mouvement\n",
    "        motion = self.motions[index]  # [Frames, Joints, Coordinates]\n",
    "\n",
    "        # Récupérer **les phrases associées** à cet ID\n",
    "        text_list = self.text_descriptions[index]\n",
    "        \n",
    "        if self.multiple_sentences:\n",
    "            # **Concaténer les phrases** en une seule avec un séparateur \". \"\n",
    "            text = \" \".join(text_list)\n",
    "        else:\n",
    "            # **Prendre uniquement la première phrase**\n",
    "            text = text_list[0]\n",
    "\n",
    "        # Tokeniser le texte\n",
    "        tokenized_text = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"motion\": torch.tensor(motion, dtype=torch.float32),  # Convertir en tenseur PyTorch\n",
    "            \"input_ids\": tokenized_text[\"input_ids\"].squeeze(0),  \n",
    "            \"attention_mask\": tokenized_text[\"attention_mask\"].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appel des descriptions/Tokenizer/initialisation du dataset MotionText/Vérification du dataset et des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:15:41.381240Z",
     "iopub.status.busy": "2025-01-31T06:15:41.380952Z",
     "iopub.status.idle": "2025-01-31T06:15:43.121211Z",
     "shell.execute_reply": "2025-01-31T06:15:43.120333Z",
     "shell.execute_reply.started": "2025-01-31T06:15:41.381217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Ajouter un token de padding explicite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:15:45.304872Z",
     "iopub.status.busy": "2025-01-31T06:15:45.304525Z",
     "iopub.status.idle": "2025-01-31T06:18:35.920112Z",
     "shell.execute_reply": "2025-01-31T06:18:35.919393Z",
     "shell.execute_reply.started": "2025-01-31T06:15:45.304846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Charger les descriptions textuelles pour chaque dataset train/val\n",
    "train_descriptions = load_text_descriptions(text_data_dir, train_indices, max_sentences=3)\n",
    "val_descriptions = load_text_descriptions(text_data_dir, val_indices,max_sentences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:18:38.980930Z",
     "iopub.status.busy": "2025-01-31T06:18:38.980611Z",
     "iopub.status.idle": "2025-01-31T06:18:39.227588Z",
     "shell.execute_reply": "2025-01-31T06:18:39.226504Z",
     "shell.execute_reply.started": "2025-01-31T06:18:38.980905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Vérifier un échantillon aléatoire\n",
    "sample_ids = random.sample(list(train_descriptions.keys()), 5)  # Prendre 5 IDs au hasard\n",
    "\n",
    "print(\"\\n **Vérification des descriptions chargées**\")\n",
    "for idx in sample_ids:\n",
    "    print(f\"\\n ID {idx} :\")\n",
    "    for i, sentence in enumerate(train_descriptions[idx]):\n",
    "        print(f\"   ➝ Phrase {i+1} : {sentence}\")\n",
    "\n",
    "# Vérifier le nombre moyen de phrases par séquence\n",
    "num_sentences = [len(desc) for desc in train_descriptions.values()]\n",
    "print(f\"\\n Moyenne de phrases par ID : {sum(num_sentences) / len(num_sentences):.2f}\")\n",
    "print(f\" Distribution : {set(num_sentences)} phrases par ID (attendu : {max_sentences})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:18:42.492947Z",
     "iopub.status.busy": "2025-01-31T06:18:42.492564Z",
     "iopub.status.idle": "2025-01-31T06:18:42.565418Z",
     "shell.execute_reply": "2025-01-31T06:18:42.564712Z",
     "shell.execute_reply.started": "2025-01-31T06:18:42.492920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Associer chaque mouvement à son ID de texte (en prenant toutes les phrases)\n",
    "indices_list = list(train_descriptions.keys())[:len(train_tensor)]  # Garder seulement les IDs qui existent\n",
    "indices_list_val = list(val_descriptions.keys())[:len(val_tensor)] \n",
    "\n",
    "#  Créer un dictionnaire {id_text: motion_tensor[i]} pour TRAIN\n",
    "train_tensor_filtered = {idx: train_tensor[i] for i, idx in enumerate(indices_list)}\n",
    "train_descriptions_filtered = {idx: train_descriptions[idx] for idx in indices_list}  #  Prendre toutes les phrases !\n",
    "\n",
    "#  Créer un dictionnaire {id_text: motion_tensor[i]} pour VALIDATION\n",
    "val_tensor_filtered = {idx: val_tensor[i] for i, idx in enumerate(indices_list_val)}\n",
    "val_descriptions_filtered = {idx: val_descriptions[idx] for idx in indices_list_val}  #  Prendre toutes les phrases !\n",
    "\n",
    "print(f\" Correspondance des indices corrigée pour TRAIN : {len(train_tensor_filtered)} séquences\")\n",
    "print(f\" Correspondance des indices corrigée pour VALIDATION : {len(val_tensor_filtered)} séquences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:18:45.735610Z",
     "iopub.status.busy": "2025-01-31T06:18:45.735299Z",
     "iopub.status.idle": "2025-01-31T06:18:45.741901Z",
     "shell.execute_reply": "2025-01-31T06:18:45.740929Z",
     "shell.execute_reply.started": "2025-01-31T06:18:45.735584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Créer le dataset avec les **données filtrées et plusieurs phrases**\n",
    "train_dataset = MotionTextDataset(train_tensor_filtered, train_descriptions_filtered, tokenizer, multiple_sentences=True)\n",
    "val_dataset = MotionTextDataset(val_tensor_filtered, val_descriptions_filtered, tokenizer, multiple_sentences=True)\n",
    "\n",
    "print(f\" Dataset d'entraînement : {len(train_dataset)} exemples\")\n",
    "print(f\" Dataset de validation : {len(val_dataset)} exemples\")\n",
    "\n",
    "batch_size = 64  # Ajustable selon la mémoire GPU\n",
    "\n",
    "# Créer les DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\" DataLoaders créés avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:18:47.963289Z",
     "iopub.status.busy": "2025-01-31T06:18:47.962975Z",
     "iopub.status.idle": "2025-01-31T06:18:47.978247Z",
     "shell.execute_reply": "2025-01-31T06:18:47.977579Z",
     "shell.execute_reply.started": "2025-01-31T06:18:47.963260Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def verify_text_descriptions(text_descriptions):\n",
    "    \"\"\"\n",
    "    Vérifie que chaque ID possède bien 3 phrases.\n",
    "    \"\"\"\n",
    "    count_per_id = {idx: len(texts) for idx, texts in text_descriptions.items()}\n",
    "    \n",
    "    #  Vérifier combien d'IDs ont exactement 3 phrases\n",
    "    total_ids = len(count_per_id)\n",
    "    ids_with_3_sentences = sum(1 for count in count_per_id.values() if count == 3)\n",
    "    \n",
    "    print(f\" Vérification des descriptions textuelles :\")\n",
    "    print(f\"  - Nombre total d'IDs : {total_ids}\")\n",
    "    print(f\"  - IDs avec exactement 3 phrases : {ids_with_3_sentences} ({(ids_with_3_sentences / total_ids) * 100:.2f}%)\")\n",
    "    \n",
    "    #  Afficher quelques exemples\n",
    "    print(\"\\n Exemples de descriptions :\")\n",
    "    for i, (idx, texts) in enumerate(text_descriptions.items()):\n",
    "        print(f\"  - ID {idx}: {texts}\")\n",
    "        if i == 4:  # Afficher seulement les 5 premiers exemples\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Tester sur les descriptions de train et validation\n",
    "verify_text_descriptions(train_descriptions)\n",
    "verify_text_descriptions(val_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:18:50.555484Z",
     "iopub.status.busy": "2025-01-31T06:18:50.555195Z",
     "iopub.status.idle": "2025-01-31T06:18:51.023134Z",
     "shell.execute_reply": "2025-01-31T06:18:51.022167Z",
     "shell.execute_reply.started": "2025-01-31T06:18:50.555461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Vérification d'un batch\n",
    "for batch in train_loader:\n",
    "    print(\"Shape des séquences de mouvements :\", batch[\"motion\"].shape)  # [Batch, Frames, Joints, Coordinates]\n",
    "    print(\"Shape des tokens textuels :\", batch[\"input_ids\"].shape)  # [Batch, Max_Length]\n",
    "    print(\"Shape des masques d'attention :\", batch[\"attention_mask\"].shape)  # [Batch, Max_Length]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:18:52.947936Z",
     "iopub.status.busy": "2025-01-31T06:18:52.947580Z",
     "iopub.status.idle": "2025-01-31T06:19:06.182321Z",
     "shell.execute_reply": "2025-01-31T06:19:06.181567Z",
     "shell.execute_reply.started": "2025-01-31T06:18:52.947907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def verify_token_lengths(text_descriptions, tokenizer, max_length=50):\n",
    "    \"\"\"\n",
    "    Vérifie le nombre de tokens générés par phrase et s'assure qu'ils ne dépassent pas max_length.\n",
    "    \"\"\"\n",
    "    token_counts = []  # Stocke les longueurs des tokens\n",
    "\n",
    "    print(\" Vérification du nombre de tokens par phrase...\")\n",
    "    \n",
    "    for idx, sentences in text_descriptions.items():\n",
    "        for sentence in sentences:  # On vérifie toutes les phrases associées à un ID\n",
    "            tokens = tokenizer(sentence, truncation=False, padding=False)[\"input_ids\"]\n",
    "            token_counts.append(len(tokens))\n",
    "    \n",
    "    #  Statistiques globales\n",
    "    print(f\" Nombre total de phrases tokenisées : {len(token_counts)}\")\n",
    "    print(f\" Longueur moyenne des phrases tokenisées : {sum(token_counts) / len(token_counts):.2f}\")\n",
    "    print(f\" Nombre de phrases dépassant {max_length} tokens : {sum(1 for t in token_counts if t > max_length)}\")\n",
    "\n",
    "    #  Afficher quelques exemples\n",
    "    print(\"\\n Exemples de phrases tokenisées :\")\n",
    "    for i, sentence in enumerate(list(text_descriptions.values())[0][:3]):  # 3 phrases du premier ID\n",
    "        tokens = tokenizer(sentence, truncation=False, padding=False)[\"input_ids\"]\n",
    "        print(f\"  - Phrase {i+1} ({len(tokens)} tokens) : {sentence}\")\n",
    "        print(f\"  - Tokens : {tokens}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Tester sur le dataset de TRAIN et VALIDATION\n",
    "verify_token_lengths(train_descriptions, tokenizer, max_length=50)\n",
    "verify_token_lengths(val_descriptions, tokenizer, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:19:06.224575Z",
     "iopub.status.busy": "2025-01-31T06:19:06.224308Z",
     "iopub.status.idle": "2025-01-31T06:19:06.631014Z",
     "shell.execute_reply": "2025-01-31T06:19:06.630081Z",
     "shell.execute_reply.started": "2025-01-31T06:19:06.224551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def verify_attention_masks(train_loader, tokenizer):\n",
    "    \"\"\"\n",
    "    Vérifie que les tokens de padding sont bien ignorés par l'attention mask.\n",
    "    \"\"\"\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        #  Vérifier si des tokens padding existent dans input_ids\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        num_pad_tokens = (input_ids == pad_token_id).sum().item()\n",
    "\n",
    "        print(f\" Vérification de l'attention mask :\")\n",
    "        print(f\"  - Nombre total de tokens dans le batch : {input_ids.numel()}\")\n",
    "        print(f\"  - Nombre de tokens de padding détectés : {num_pad_tokens}\")\n",
    "        print(f\"  - Exemple de Input IDs : {input_ids[0].tolist()}\")\n",
    "        print(f\"  - Exemple de Attention Mask : {attention_mask[0].tolist()}\")\n",
    "        break  # On affiche seulement le premier batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Tester sur le TRAIN LOADER\n",
    "verify_attention_masks(train_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Architecture du modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:21:16.853364Z",
     "iopub.status.busy": "2025-01-31T06:21:16.853061Z",
     "iopub.status.idle": "2025-01-31T06:21:16.862907Z",
     "shell.execute_reply": "2025-01-31T06:21:16.862036Z",
     "shell.execute_reply.started": "2025-01-31T06:21:16.853341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MotionEncoderTransformer(nn.Module):\n",
    "    def __init__(self, joint_dim, coord_dim, hidden_dim=256, embedding_dim=512, hidden_size=768, num_layers=4, num_heads=8, ff_dim=None, dropout=0.1):\n",
    "        super(MotionEncoderTransformer, self).__init__()\n",
    "\n",
    "        self.noise_factor = 0.05\n",
    "        ff_dim = ff_dim or (hidden_dim * 4)  #  Défaut : 4x hidden_dim si non spécifié\n",
    "\n",
    "        #  Convolutions 1D pour capturer les motifs locaux\n",
    "        self.conv1 = nn.Conv1d(in_channels=joint_dim * coord_dim, out_channels=hidden_dim, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        #  Normalisation & Dropout\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout_conv = nn.Dropout(dropout)\n",
    "\n",
    "        #  TransformerEncoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=ff_dim,  #  Utilisation de ff_dim ici\n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        #  Fully Connected\n",
    "        self.fc = nn.Linear(hidden_dim, embedding_dim)  \n",
    "\n",
    "        #  Projection vers GPT-2\n",
    "        self.projection = nn.Linear(embedding_dim, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        #  Dropout final\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, J, C = x.shape\n",
    "        x = x.view(B, T, J * C).permute(0, 2, 1)  # [Batch, Features, Time]\n",
    "\n",
    "        #  Convolutions + Activation + Normalisation\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.layer_norm1(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.dropout_conv(x)\n",
    "\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.layer_norm2(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.dropout_conv(x)\n",
    "\n",
    "        #  Passage dans le Transformer Encoder\n",
    "        x = x.permute(0, 2, 1)  # [Batch, Time, Features]\n",
    "        x = self.transformer_encoder(x)  # [Batch, Time, Hidden_dim]\n",
    "        x = x[:, -1, :]  # Prendre le dernier état de la séquence (comme LSTM)\n",
    "\n",
    "        #  Fully Connected + Activation\n",
    "        x = self.fc(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.02)  \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        #  Projection vers GPT-2\n",
    "        x = self.projection(x)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        #  Ajout de bruit pour stabiliser\n",
    "        noise = torch.randn_like(x) * self.noise_factor\n",
    "        x = x + noise\n",
    "\n",
    "        #  Normalisation L2 finale\n",
    "        x = F.normalize(x, p=2, dim=-1) * 10\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:21:20.450906Z",
     "iopub.status.busy": "2025-01-31T06:21:20.450601Z",
     "iopub.status.idle": "2025-01-31T06:21:21.537833Z",
     "shell.execute_reply": "2025-01-31T06:21:21.536945Z",
     "shell.execute_reply.started": "2025-01-31T06:21:20.450884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained(\"gpt2\", add_cross_attention=True) #  Charger GPT-2 avec Cross-Attention\n",
    "text_decoder = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config) # Charger sur CPU d'abord, puis transférer sur GPU\n",
    "\n",
    "text_decoder = text_decoder.to(\"cuda\")\n",
    "text_decoder.resize_token_embeddings(len(tokenizer)) #  Mettre à jour le modèle pour prendre en compte le nouveau token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:21:23.213512Z",
     "iopub.status.busy": "2025-01-31T06:21:23.213198Z",
     "iopub.status.idle": "2025-01-31T06:21:23.218943Z",
     "shell.execute_reply": "2025-01-31T06:21:23.218070Z",
     "shell.execute_reply.started": "2025-01-31T06:21:23.213484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextDecoderTransformer(nn.Module):\n",
    "    def __init__(self, latent_dim=768, gpt2_model=\"gpt2\"):\n",
    "        super(TextDecoderTransformer, self).__init__()\n",
    "\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_model)\n",
    "\n",
    "        #  Ajout d'une projection vers GPT-2\n",
    "        self.latent_to_embedding = nn.Linear(latent_dim, self.gpt2.config.n_embd)\n",
    "        self.layer_norm = nn.LayerNorm(self.gpt2.config.n_embd)\n",
    "\n",
    "        #  Ajout d’un Cross-Attention Layer\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=self.gpt2.config.n_embd, num_heads=8, batch_first=True)\n",
    "\n",
    "    def forward(self, latent_vector, input_ids, attention_mask):\n",
    "        #  Projection et normalisation du latent vector\n",
    "        transformed_vector = self.latent_to_embedding(latent_vector)\n",
    "        transformed_vector = self.layer_norm(transformed_vector).unsqueeze(1)  # [Batch, 1, Embedding_dim]\n",
    "\n",
    "        #  Passage dans Cross-Attention avant GPT-2\n",
    "        hidden_state, _ = self.cross_attention(transformed_vector, transformed_vector, transformed_vector)\n",
    "\n",
    "        #  Passage dans GPT-2\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask, encoder_hidden_states=hidden_state)\n",
    "        return outputs.logits  # [Batch, Seq_Length, Vocab_Size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:21:25.548047Z",
     "iopub.status.busy": "2025-01-31T06:21:25.547757Z",
     "iopub.status.idle": "2025-01-31T06:21:25.552931Z",
     "shell.execute_reply": "2025-01-31T06:21:25.552193Z",
     "shell.execute_reply.started": "2025-01-31T06:21:25.548024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU disponible :\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Aucun GPU disponible. Le calcul se fera sur le CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T06:21:25.808310Z",
     "iopub.status.busy": "2025-01-31T06:21:25.808072Z",
     "iopub.status.idle": "2025-01-31T06:21:25.813011Z",
     "shell.execute_reply": "2025-01-31T06:21:25.812230Z",
     "shell.execute_reply.started": "2025-01-31T06:21:25.808289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Vérifie si le GPU est disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Appareil utilisé :\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entrainement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:36:29.171089Z",
     "iopub.status.busy": "2025-01-31T07:36:29.170763Z",
     "iopub.status.idle": "2025-01-31T07:36:29.184701Z",
     "shell.execute_reply": "2025-01-31T07:36:29.183738Z",
     "shell.execute_reply.started": "2025-01-31T07:36:29.171057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mean_pooling(hidden_states, mask):\n",
    "    \"\"\"Moyenne des embeddings cachés en tenant compte du mask\"\"\"\n",
    "    mask_expanded = mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "    pooled = torch.sum(hidden_states * mask_expanded, dim=1) / (mask_expanded.sum(dim=1) + 1e-9)\n",
    "    return pooled.squeeze(1) if pooled.dim() == 3 else pooled\n",
    "\n",
    "def train_autoencoder(motion_encoder, text_decoder, train_loader, tokenizer, device=\"cuda\", num_epochs=10, lr=1e-4, gradient_accumulation_steps=1):\n",
    "    \n",
    "    #  Déplacer les modèles sur GPU et les mettre en mode entraînement\n",
    "    motion_encoder.to(device).train()\n",
    "    text_decoder.to(device).train()\n",
    "\n",
    "    #  Optimiseur (AdamW avec régularisation plus forte)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(motion_encoder.parameters()) + list(text_decoder.parameters()), \n",
    "        lr=lr, weight_decay=0.02  #  Légère augmentation du weight decay pour régularisation\n",
    "    )\n",
    "\n",
    "    #  Scheduler dynamique (ReduceLROnPlateau)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=2, verbose=True  #  Ajustement plus progressif du LR\n",
    "    )\n",
    "\n",
    "    #  Fonction de perte\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    #  Normalisation des embeddings pour stabiliser\n",
    "    embedding_norm = torch.nn.LayerNorm(768).to(device)\n",
    "\n",
    "    #  Geler GPT-2 pendant les **4 premières époques**\n",
    "    for param in text_decoder.parameters():\n",
    "        param.requires_grad = False  \n",
    "\n",
    "    beta = 0.98  #  Moving Average pour la stabilisation de la loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        running_loss = 0  \n",
    "\n",
    "        #  Barre de progression `tqdm`\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\" Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, batch in progress_bar:\n",
    "            try:\n",
    "                #  Charger les données sur GPU\n",
    "                motions = batch[\"motion\"].to(device, non_blocking=True)\n",
    "                input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "                #  Encodage du mouvement avec Transformer Encoder\n",
    "                motion_embeddings = motion_encoder(motions)  # [Batch, Time, Hidden]\n",
    "                if motion_embeddings.dim() == 3:  \n",
    "                    motion_embeddings = mean_pooling(motion_embeddings, attention_mask)  # [Batch, Hidden]\n",
    "\n",
    "                motion_embeddings = embedding_norm(motion_embeddings)  # [Batch, Hidden]\n",
    "                motion_embeddings = motion_embeddings.unsqueeze(1)  # [Batch, 1, Hidden] pour GPT-2\n",
    "\n",
    "                #  Scheduled Sampling amélioré\n",
    "                if epoch > 2 and loss.item() < 3.8:  #  Déclenchement basé sur la perte\n",
    "                    with torch.no_grad():\n",
    "                        sampled_ids = torch.argmax(text_decoder(\n",
    "                            input_ids=input_ids[:, :-1], \n",
    "                            attention_mask=attention_mask[:, :-1], \n",
    "                            encoder_hidden_states=motion_embeddings\n",
    "                        ).logits, dim=-1)\n",
    "                    \n",
    "                    mask = torch.rand_like(input_ids[:, :-1].float()) < 0.10  # 🔹 10% remplacés par la prédiction\n",
    "                    input_ids[:, :-1][mask] = sampled_ids[mask]\n",
    "\n",
    "                #  Passage dans GPT-2 (décodeur)\n",
    "                outputs = text_decoder(\n",
    "                    input_ids=input_ids[:, :-1],  \n",
    "                    attention_mask=attention_mask[:, :-1],\n",
    "                    encoder_hidden_states=motion_embeddings,\n",
    "                    labels=input_ids[:, 1:]  \n",
    "                )\n",
    "\n",
    "                logits = outputs.logits  # [Batch, Seq_Length, Vocab_Size]\n",
    "\n",
    "                #  Calcul de la perte\n",
    "                loss = criterion(logits.permute(0, 2, 1), input_ids[:, 1:])\n",
    "\n",
    "                #  Backpropagation avec Gradient Accumulation\n",
    "                loss.backward()\n",
    "                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(motion_encoder.parameters(), max_norm=5.0)  \n",
    "                    torch.nn.utils.clip_grad_norm_(text_decoder.parameters(), max_norm=5.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                running_loss = beta * running_loss + (1 - beta) * loss.item()\n",
    "                smoothed_loss = running_loss / (1 - beta ** (batch_idx + 1))\n",
    "\n",
    "                #  Mise à jour de la barre de progression\n",
    "                progress_bar.set_postfix(loss=f\"{smoothed_loss:.4f}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\" Erreur au batch {batch_idx + 1}: {e}\")\n",
    "                continue  \n",
    "\n",
    "        #  Mise à jour du scheduler\n",
    "        scheduler.step(total_loss / max(1, len(train_loader)))  #  Réduction du LR uniquement si la perte ne descend pas\n",
    "\n",
    "        #  Débloquer GPT-2 après 4 epochs\n",
    "        if epoch == 4:\n",
    "            for param in text_decoder.parameters():\n",
    "                param.requires_grad = True  \n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] *= 0.5  #  Réduction du LR avant fine-tuning de GPT-2\n",
    "\n",
    "        #  Sauvegarde automatique du modèle\n",
    "        torch.save(motion_encoder.state_dict(), f\"motion_encoder_epoch_{epoch+1}.pth\")\n",
    "        torch.save(text_decoder.state_dict(), f\"text_decoder_epoch_{epoch+1}.pth\")\n",
    "        print(f\" Modèle sauvegardé à l'époque {epoch+1} !\")\n",
    "\n",
    "        #  Afficher la perte moyenne par époque\n",
    "        avg_loss = total_loss / max(1, len(train_loader))  \n",
    "        print(f\" Époque [{epoch+1}/{num_epochs}] - Perte moyenne : {avg_loss:.4f}\")\n",
    "\n",
    "    print(\" Entraînement terminé avec succès ! \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation de l'encoder et lancement de l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:36:31.527585Z",
     "iopub.status.busy": "2025-01-31T07:36:31.527290Z",
     "iopub.status.idle": "2025-01-31T07:36:31.633736Z",
     "shell.execute_reply": "2025-01-31T07:36:31.632819Z",
     "shell.execute_reply.started": "2025-01-31T07:36:31.527561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "motion_encoder = MotionEncoderTransformer(\n",
    "    joint_dim=22,    # Nombre de jointures\n",
    "    coord_dim=3,     # Coordonnées x, y, z\n",
    "    hidden_dim=512,  #  Augmenté pour plus de capacité de représentation\n",
    "    num_layers=6,    #  6 couches de Transformer Encoder\n",
    "    num_heads=8,     #  Multi-head attention (8 têtes)\n",
    "    ff_dim=1024,     #  Feedforward dimension interne\n",
    "    dropout=0.1      # Régularisation\n",
    ").to(\"cuda\")  # Envoi sur le GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:36:32.251654Z",
     "iopub.status.busy": "2025-01-31T07:36:32.251314Z",
     "iopub.status.idle": "2025-01-31T07:36:32.263549Z",
     "shell.execute_reply": "2025-01-31T07:36:32.262908Z",
     "shell.execute_reply.started": "2025-01-31T07:36:32.251599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Simuler un batch de mouvements (batch_size=4, frames=150, joints=22, coords=3)\n",
    "dummy_motion = torch.randn(4, 150, 22, 3).to(\"cuda\")\n",
    "\n",
    "# Passer dans le MotionEncoder\n",
    "with torch.no_grad():\n",
    "    output_embedding = motion_encoder(dummy_motion)\n",
    "\n",
    "print(f\" MotionEncoder Output: {output_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réinitialisation des poids de l'encodeur pour entraînement complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:36:32.629082Z",
     "iopub.status.busy": "2025-01-31T07:36:32.628768Z",
     "iopub.status.idle": "2025-01-31T07:36:32.636978Z",
     "shell.execute_reply": "2025-01-31T07:36:32.636083Z",
     "shell.execute_reply.started": "2025-01-31T07:36:32.629053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reset_model_weights(model):\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, 'reset_parameters'):\n",
    "            module.reset_parameters()\n",
    "\n",
    "# Réinitialiser uniquement le MotionEncoder\n",
    "reset_model_weights(motion_encoder)\n",
    "\n",
    "print(\" MotionEncoder réinitialisé, GPT-2 conservé ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:36:35.732603Z",
     "iopub.status.busy": "2025-01-31T07:36:35.732296Z",
     "iopub.status.idle": "2025-01-31T07:36:36.186524Z",
     "shell.execute_reply": "2025-01-31T07:36:36.185560Z",
     "shell.execute_reply.started": "2025-01-31T07:36:35.732577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.keys())  # Doit contenir \"motion\", \"input_ids\", \"attention_mask\"\n",
    "    break\n",
    "\n",
    "assert next(motion_encoder.parameters()).is_cuda, \"MotionEncoder n'est pas sur GPU !\"\n",
    "assert next(text_decoder.parameters()).is_cuda, \"TextDecoder n'est pas sur GPU !\"\n",
    "assert tokenizer.pad_token_id is not None, \" pad_token_id non défini dans le tokenizer !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T07:36:37.148784Z",
     "iopub.status.busy": "2025-01-31T07:36:37.148408Z",
     "iopub.status.idle": "2025-01-31T07:36:37.152955Z",
     "shell.execute_reply": "2025-01-31T07:36:37.152031Z",
     "shell.execute_reply.started": "2025-01-31T07:36:37.148750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "train_autoencoder(motion_encoder, text_decoder, train_loader, tokenizer, device=\"cuda\", num_epochs=10) # Quasi complète convergence dès 10 épochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sauvegarde du modèle final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T09:08:04.186215Z",
     "iopub.status.busy": "2025-01-31T09:08:04.185908Z",
     "iopub.status.idle": "2025-01-31T09:08:05.976515Z",
     "shell.execute_reply": "2025-01-31T09:08:05.975604Z",
     "shell.execute_reply.started": "2025-01-31T09:08:04.186188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Sauvegarder le motion_encoder\n",
    "motion_encoder_path = \"/kaggle/working/motion_encoder\"\n",
    "torch.save(motion_encoder.state_dict(), motion_encoder_path)\n",
    "print(f\" Motion Encoder sauvegardé sous : {motion_encoder_path}\")\n",
    "\n",
    "#  Sauvegarder le text_decoder\n",
    "text_decoder_path = \"/kaggle/working/text_decoder\"\n",
    "torch.save(text_decoder.state_dict(), text_decoder_path)\n",
    "print(f\" Text Decoder sauvegardé sous : {text_decoder_path}\")\n",
    "\n",
    "#  Sauvegarder le tokenizer (Hugging Face)\n",
    "tokenizer_path = \"/kaggle/working/tokenizer\"\n",
    "tokenizer.save_pretrained(tokenizer_path)\n",
    "print(f\" Tokenizer GPT-2 sauvegardé sous : {tokenizer_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Génération des prédictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:07:37.892877Z",
     "iopub.status.busy": "2025-01-31T10:07:37.892569Z",
     "iopub.status.idle": "2025-01-31T10:07:37.900516Z",
     "shell.execute_reply": "2025-01-31T10:07:37.899795Z",
     "shell.execute_reply.started": "2025-01-31T10:07:37.892853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_descriptions(motion_encoder, text_decoder, tokenizer, tensors, indices, device=\"cuda\", max_new_tokens=17, mode=\"validation\"):\n",
    "    #  Mode évaluation\n",
    "    motion_encoder.eval()\n",
    "    text_decoder.eval()\n",
    "\n",
    "    generated_descriptions = {}\n",
    "\n",
    "    # Liste des conjonctions et prépositions qui indiquent une phrase incomplète\n",
    "    invalid_endings = [\"and\", \"then\", \"as\", \"while\", \"with\", \"before\", \"after\", \"but\", \"so\"]\n",
    "\n",
    "    with torch.no_grad():  # Pas de calcul de gradient pour l'inférence\n",
    "        for i, (motion, idx) in enumerate(zip(tensors, indices)):\n",
    "            motion = motion.unsqueeze(0).to(device)  # Ajouter une dimension batch\n",
    "\n",
    "            #  1. Encoder le mouvement\n",
    "            motion_embedding = motion_encoder(motion)  # [1, 768]\n",
    "            motion_embedding = motion_embedding.unsqueeze(1)  # GPT-2 attend [Batch, 1, Hidden]\n",
    "\n",
    "            print(f\"motion_embedding {i}: mean={motion_embedding.mean().item()}, std={motion_embedding.std().item()}\")\n",
    "\n",
    "            #  2. Définir un prompt minimal\n",
    "            start_prompt = \"A person\"\n",
    "            input_ids = tokenizer(start_prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "            #  3. Générer le texte avec des paramètres optimisés\n",
    "            output = text_decoder.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_new_tokens,  # Plus de tokens pour éviter une coupure trop tôt\n",
    "                num_beams=5,  # Augmente la cohérence\n",
    "                encoder_hidden_states=motion_embedding,  \n",
    "                attention_mask=input_ids.new_ones(input_ids.shape),\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=True,  \n",
    "                top_p=0.85,  # Réduction de la diversité excessive\n",
    "                temperature=1.8,  # Moins de hasard pour de meilleures phrases\n",
    "                repetition_penalty=2.0,  # Pénalise les répétitions sans supprimer des structures utiles\n",
    "                no_repeat_ngram_size=4  # Évite des répétitions longues\n",
    "            )\n",
    "\n",
    "            #  4. Décoder les tokens\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "            #  Correction pour éviter les répétitions et phrases mal formées\n",
    "            if not generated_text.lower().startswith(\"a person\"):\n",
    "                generated_text = \"A person \" + generated_text  # Ajout forcé\n",
    "\n",
    "            #  Garder uniquement la première phrase correcte\n",
    "            sentences = generated_text.split(\".\")\n",
    "            generated_text = sentences[0].strip() + \".\"\n",
    "\n",
    "            #  Assurer une bonne fin de phrase\n",
    "            last_word = generated_text.split()[-1].lower()\n",
    "            if last_word in invalid_endings:  # Si la phrase finit mal, on force une fin correcte\n",
    "                generated_text += \" They complete the movement.\"\n",
    "\n",
    "            #  Associer à l'ID du mouvement\n",
    "            generated_descriptions[idx] = generated_text\n",
    "            print(f\" {mode.capitalize()} {i+1}/{len(tensors)} - Généré : {generated_text}\")\n",
    "\n",
    "    return generated_descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On sort les prédictions du jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:07:39.346487Z",
     "iopub.status.busy": "2025-01-31T10:07:39.346012Z",
     "iopub.status.idle": "2025-01-31T10:11:57.863662Z",
     "shell.execute_reply": "2025-01-31T10:11:57.862933Z",
     "shell.execute_reply.started": "2025-01-31T10:07:39.346440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Générer les descriptions pour le test set\n",
    "\n",
    "test_descriptions = generate_descriptions(\n",
    "    motion_encoder, text_decoder, tokenizer, test_tensor, test_indices, device=\"cuda\",mode=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T10:16:24.159457Z",
     "iopub.status.busy": "2025-01-31T10:16:24.159147Z",
     "iopub.status.idle": "2025-01-31T10:16:24.167267Z",
     "shell.execute_reply": "2025-01-31T10:16:24.166568Z",
     "shell.execute_reply.started": "2025-01-31T10:16:24.159433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sauvegarder les prédictions brutes dans un CSV\n",
    "backup_df = pd.DataFrame({\n",
    "    'id': list(test_descriptions.keys()),\n",
    "    'text': list(test_descriptions.values())\n",
    "})\n",
    "\n",
    "# Enregistrer le fichier CSV\n",
    "backup_csv_path = \"/kaggle/working/test_descriptions_backup9.csv\"\n",
    "backup_df.to_csv(backup_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10929179,
     "sourceId": 91969,
     "sourceType": "competition"
    },
    {
     "sourceId": 219994917,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
